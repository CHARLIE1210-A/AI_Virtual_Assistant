{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa14e2a-fa49-4a1c-9ead-e5f8f1318356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91789\\anaconda3\\envs\\vbva\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a89f4bd-683a-474e-8b90-bbd6f19795d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Result: {'verified': True, 'distance': 0.14601927550907468, 'threshold': 0.68, 'model': 'ArcFace', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 999, 'y': 437, 'w': 1888, 'h': 1888, 'left_eye': (2247, 1160), 'right_eye': (1564, 1255)}, 'img2': {'x': 1276, 'y': 738, 'w': 1150, 'h': 1150, 'left_eye': (2041, 1186), 'right_eye': (1631, 1252)}}, 'time': 50.09}\n"
     ]
    }
   ],
   "source": [
    "# Verify two faces with the ArcFace model\n",
    "result = DeepFace.verify(img1_path=\"IMG_0129.JPG\", img2_path=\"IMG_0121.JPG\", model_name=\"ArcFace\")\n",
    "print(\"Verification Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca832806-5b26-4d72-913e-4a38473f72df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze a face for emotion, age, and gender\n",
    "analysis = DeepFace.analyze(img_path=\"IMG_0129.JPG\", actions=[\"emoti on\", \"age\", \"gender\"])\n",
    "print(\"Analysis Result:\", analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e6e7d2-b46e-45ea-8807-ca799fe07d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91789\\anaconda3\\envs\\vbva\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# Pause between frames to simulate capturing frames in a short duration\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Start capturing video from the webcam\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m face_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "\n",
    "# Path to existing face database\n",
    "database_path = \"./\"\n",
    "# Path to store newly detected unknown faces\n",
    "unknown_faces_path = \"./\"\n",
    "\n",
    "# Ensure the unknown faces directory exists\n",
    "os.makedirs(unknown_faces_path, exist_ok=True)\n",
    "\n",
    "# Number of frames to capture for an unknown face\n",
    "frames_to_capture = 5\n",
    "\n",
    "def save_unknown_face(frame, face_id):\n",
    "    # Save the image of the unknown face\n",
    "    face_path = os.path.join(unknown_faces_path, f\"unknown_{face_id}.jpg\")\n",
    "    cv2.imwrite(face_path, frame)\n",
    "    print(f\"Saved unknown face at {face_path}\")\n",
    "\n",
    "def capture_unknown_face_frames(frame, face_id):\n",
    "    # Capture a few frames of the unknown face and store them\n",
    "    for i in range(frames_to_capture):\n",
    "        face_path = os.path.join(unknown_faces_path, f\"unknown_{face_id}_{i}.jpg\")\n",
    "        cv2.imwrite(face_path, frame)\n",
    "        print(f\"Captured frame {i+1} for unknown face at {face_path}\")\n",
    "        time.sleep(0.5)  # Pause between frames to simulate capturing frames in a short duration\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_id = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Perform face detection and comparison with the database using DeepFace\n",
    "        result = DeepFace.find(img_path=frame, db_path=database_path, detector_backend=\"mtcnn\", enforce_detection=False)\n",
    "\n",
    "        # If no match is found, capture a few frames and store the unknown face\n",
    "        if result.empty:\n",
    "            print(\"Unknown face detected!\")\n",
    "            capture_unknown_face_frames(frame, face_id)\n",
    "            face_id += 1\n",
    "\n",
    "        # Use DeepFace.stream() to display the image stream with detected faces (if any)\n",
    "        DeepFace.stream(db_path=database_path, detector_backend=\"mtcnn\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Display the live webcam feed with OpenCV\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c22ed1b-c073-480a-a1c3-9dae3d9bbf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Analysis: [{'emotion': {'angry': 7.569800342110966e-08, 'disgust': 6.656790780236072e-17, 'fear': 0.0007183551588241244, 'happy': 99.64566826820374, 'sad': 0.0010726932487159502, 'surprise': 1.1499799029701663e-08, 'neutral': 0.3525430103763938}, 'dominant_emotion': 'happy', 'region': {'x': 329, 'y': 176, 'w': 215, 'h': 215, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.93}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81a2b5b-9029-472f-b698-360adc525263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: list indices must be integers or slices, not str\n",
      "Error: list indices must be integers or slices, not str\n",
      "Error: list indices must be integers or slices, not str\n",
      "Error: list indices must be integers or slices, not str\n",
      "Error: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Limit the number of frames for demonstration purposes in a notebook\n",
    "frame_count = 5\n",
    "\n",
    "for _ in range(frame_count):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Analyze the frame for emotion\n",
    "    try:\n",
    "        analysis = DeepFace.analyze(frame, actions=[\"emotion\"])\n",
    "        emotion = analysis[\"dominant_emotion\"]\n",
    "        print(\"Emotion Analysis:\", analysis)\n",
    "        \n",
    "        # Display the frame with detected emotion\n",
    "        frame_with_text = frame.copy()\n",
    "        cv2.putText(frame_with_text, f\"Emotion: {emotion}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Convert BGR to RGB for displaying with matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame_with_text, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Show the frame in the notebook\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb159a5c-6e90-4c89-b493-8c2bd6c9bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def is_valid_path(img_path):\n",
    "    return os.path.exists(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b7105-68b2-4894-9bd3-c558a90620c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(img_path):\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "    return img_path.lower().endswith(valid_extensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87953b6a-fa91-42af-9357-ffc4d50270c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def is_readable_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    return img is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c77714-db42-464d-a931-6de0c591b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_readable_image(\"unknown_face_0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1a5f7d-0471-4338-85c9-0511737ebed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image_path(img_path):\n",
    "    if not is_valid_path(img_path):\n",
    "        raise FileNotFoundError(f\"File not found: {img_path}\")\n",
    "    if not is_image_file(img_path):\n",
    "        raise ValueError(f\"Invalid image file type: {img_path}\")\n",
    "    if not is_readable_image(img_path):\n",
    "        raise ValueError(f\"File cannot be read as an image: {img_path}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51d3bdd5-7267-4f6f-96aa-ea9f415036d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is valid!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    validate_image_path(\"detected_face_3.jpg\")\n",
    "    print(\"Image is valid!\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcdb8620-df77-4e1a-9eb5-65e7ad959a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_to_supported_format(img_path, output_path=\"output.jpg\"):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img.save(output_path, format=\"JPEG\")  # Save as a compatible format\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting image: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21042818-9813-43d7-a56d-e38d60ce2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_supported_format(\"detected_face_3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351e7237-ac52-4384-8db6-0ba683e218a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, output_size=(224, 224)):\n",
    "    img = cv2.imread(img_path)\n",
    "    resized_img = cv2.resize(img, output_size)  # Resize to match model requirements\n",
    "    cv2.imwrite(\"preprocessed.jpg\", resized_img)\n",
    "    return \"preprocessed.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091fed71-bbc1-4cdb-9126-ee909fc5838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_image_as_array(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image could not be read\")\n",
    "    return np.array(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64665dee-fc46-4fb4-8c07-eef8078f2c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Exception while processing img1_path'}\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "def verify_images(img1_path, img2_path):\n",
    "    try:\n",
    "        validate_image_path(img1_path)\n",
    "        validate_image_path(img2_path)\n",
    "        result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example Usage\n",
    "img1 = \"unknown_face_0.jpg\"\n",
    "img2 = \"AshutoshSingh1.png\"\n",
    "result = verify_images(img1, img2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b216dd90-a411-47c2-aa58-6a4804bb47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to capture frame.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # 0 for the default webcam\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    \n",
    "\n",
    "# Discard initial frames for stabilization\n",
    "for _ in range(10):\n",
    "    cap.read()   \n",
    "           \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Process the frame for face detection and recognition\n",
    "    # processor.process_frames(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Real-Time Face Detection & Recognition\", frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50a613-07f2-40da-a198-de4f3bfa5283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
